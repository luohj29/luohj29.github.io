<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="Rogers | learning in cs and math">
    <meta name="keywords" content="Rogers, luohj29, blog, cs, math, sysu,ML,Photography">
    <meta name="theme-color" content="#000000">

    <!-- Open Graph -->
    
    <meta property="og:title"
                content="HPC lab5 cuda编程的并行计算 - RogersLuo">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="高性能计算程序设计（5） 秋季2024
">
    
    <meta property="article:published_time" content=" 2024-11-13T00:00:00Z">
    
    
    
    <meta property="article:tag" content="HPC">
    
    
    <meta property="og:image" content="http://localhost:4000/img/Rogers.png">
    <meta property="og:url" content="http://localhost:4000/2024-11-13-HPClab5/">
    <meta property="og:site_name" content="RogersLuo">

    <title>HPC lab5 cuda编程的并行计算 - RogersLuo</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2024-11-13-HPClab5/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href=" /css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href=" /css/hux-blog.min.css">

    

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet"
        type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->

    <!-- diffBackground -->
    <script type="text/javascript">
        function diffBackground() {
            datetoday = new Date();
            timenow = datetoday.getTime();
            datetoday.setTime(timenow);
            thehour = datetoday.getHours();
            if (thehour >= 15 && thehour < 18)
                display = "img/index-bg.jpg";
            else if (thehour >= 18 && thehour < 21)
                display = "img/index-bg-night.jpg";
            else if (thehour >= 21 && thehour < 24)
                display = "img/home-bg-star_track.jpg";
            else if (thehour >= 0 && thehour < 3)
                display = "img/home-bg-star_track.jpg";
            else if (thehour >= 3 && thehour < 6)
                display = "img/index-bg-night.jpg";
            else if (thehour >= 6 && thehour < 12)
                display = "img/index-bg.jpg";
            else if (thehour >= 12 && thehour < 15)
                display = "img/index-bg.jpg";
            else
                display = "img/index-bg.jpg";

            var css = '<style type="text/css">';
            css += 'header.intro-header{background-image: url(\'/' + display + '\');}';
            css += '</style>';
            document.write(css);
        }
    </script>

</head>

<!-- hack iOS CSS :active style -->

<body ontouchstart="">

    <!-- Navigation -->

<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/blog">RogersLuo's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    <li>
                        <a href="/blog">Blog</a>
                    </li>
                    <li>
                        <a href="/archive">Archive</a>
                    </li>
                    <!-- 
                    
                    
                    
                    
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="/archive/">Archive</a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                     -->
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/index-bg.jpg" width="0" height="0"> -->

<!-- Post Header -->





    
    <style type="text/css">
        header.intro-header{
            position: relative;
            background-image: url('/img/index-bg.jpg');
            background: ;
        }

        
    </style>
    
    
        
    <header class="intro-header" >
        
    
    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=HPC" title="HPC">HPC</a>
                        
                    </div>
                    <h1 >HPC lab5 cuda编程的并行计算</h1>
                    
                    <h2 class="subheading" ></h2>
                    <span class="meta">Posted by RogersLuo's Blog on November 13, 2024</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    
        <div class="container">
            
            <div class="row">

                <!-- Post Container -->
                <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                    <!-- Multi-Lingual -->
                    

                    <h1 id="高性能计算程序设计5-秋季2024">高性能计算程序设计（5） 秋季2024</h1>

<p><strong><em>提交格式说明</em></strong></p>

<p>按照实验报告模板填写报告，需要提供源代码及代码描述至https://easyhpc.net/course/212。实验报告模板使用PDF格式，命名方式为高性能计算程序设计_学号_姓名。如果有问题，请发邮件至&lt;zhudp3@mail2.sysu.edu.cn、liux276@mail2.sysu.edu.cn&gt;询问细节。</p>

<h3 id="任务1"><strong>任务1：</strong></h3>

<p>通过CUDA实现通用矩阵乘法（Lab1）的并行版本，CUDA Thread Block size从32增加至512，矩阵规模从512增加至8192。</p>

<p>通用矩阵乘法（GEMM）通常定义为：</p>

\[C = AB\]

\[C_{m,n} = \sum_{n = 1}^{N}{A_{m,n}B_{n,k}}\]

<p>输入：M , N, K三个整数（512 ~8192）</p>

<p>问题描述：随机生成M*N和N*K的两个矩阵A,B,对这两个矩阵做乘法得到矩阵C。</p>

<p>输出：A,B,C三个矩阵以及矩阵计算的时间</p>

<h4 id="代码解释"><strong>代码解释</strong>:</h4>

<p>?	《grid, block》将矩阵分割到每一个cuda thread， 然后每一个thread只有其坐标在(0,0)到(m,k)（也就是目标矩阵的大小内）才有效，这样的thread遍历a的行和b的列，对应相乘加入到本地temp,然后赋值到目标矩阵，由于一一对应，所以不需要对目标矩阵加锁。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="rouge-code"><pre><span class="cm">/**
 * @brief  implemenntation of the GEMM cuda in global memory
 * 
 * @param a  the first matrix in m*n in device memory
 * @param b  the second matrix in n*k in device memory
 * @param c  the result matrix in m*k in device memory
 * @param m  matrix size
 * @param n  matrix size
 * @param k  matrix size 
 * @return the result in ptr c
 */</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_matrix_mult_gm</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
    <span class="kt">float</span> <span class="n">temp</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">)</span> <span class="c1">// Ensure bounds are within the matrix dimensions</span>
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">temp</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="n">c</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>进一步优化，可以考虑分块矩阵，以及共享内存，分块矩阵乘法在缓存命中率上有提高，cuda的cache模型是一行128字节,也就是32个float，将矩阵划分为32*32的小块，可以增加缓存读写的使用率，同时共享内存的读写也比全局内存快很多。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td class="rouge-code"><pre><span class="cm">/**
 * @brief  implemenntation of the GEMM cuda in shared memory
 *
 * @param a  the first matrix in m*n in device memory
 * @param b  the second matrix in n*k in device memory
 * @param c  the result matrix in m*k in device memory
 * @param m  matrix size
 * @param n  matrix size
 * @param k  matrix size
 * @return the result in ptr c
 */</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_matrix_mult_sm</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tile_a</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">]</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tile_b</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">]</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
    <span class="kt">float</span> <span class="n">temp</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="kt">int</span> <span class="n">block_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span> <span class="c1">// Divide the n dimension by block size</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">block_num</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// Copy the ith block into shared memory</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">a_index</span> <span class="o">=</span> <span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
            <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">a_index</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="k">else</span>
            <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">// Handle edge case for smaller matrix</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">k</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">b_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span>
            <span class="n">tile_b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">b_index</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="k">else</span>
            <span class="n">tile_b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nf">__syncthreads</span><span class="p">()</span> <span class="c1">// Synchronize threads in the block before computation</span>

        <span class="c1">// Compute the contribution for c[row][col]</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">BLOCK_SIZE</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">temp</span> <span class="o">+=</span> <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_b</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="n">__syncthreads</span><span class="p">()</span> <span class="c1">// Ensure all threads finish their computation before moving on</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">c</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>奇怪的问题：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre>        <span class="c1">// Copy the ith block into shared memory</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">a_index</span> <span class="o">=</span> <span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
            <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">a_index</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="k">else</span>
            <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">// Handle edge case for smaller matrix</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">k</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="kt">int</span> <span class="n">b_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span>
            <span class="n">tile_b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">b_index</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="k">else</span>
            <span class="n">tile_b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nf">__syncthreads</span><span class="p">()</span> <span class="c1">// Synchronize threads in the block before computation</span>

        <span class="c1">// Compute the contribution for c[row][col]</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">BLOCK_SIZE</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">temp</span> <span class="o">+=</span> <span class="n">tile_a</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">tile_b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	使用加速方法，将b矩阵转置后，本来觉得可以加快缓存读取，但是发现程序速度反而变慢了</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
</pre></td><td class="rouge-code"><pre>Testting the diy_cuda_global_mem implementation of matrix multiply
510.3349 520.4258 ... 514.6938 517.7357
508.9028 ...... ... ...... 520.2448
...... ...... ... ...... ......
507.9561 ...... ... ...... 513.7372
521.5385 526.4379 ... 520.4982 516.9260
Custom Kernel Time: 3.424504 ms Performance: 5016.746651 GFLOPS

Testting the diy_cuda_shared_mem implementation of matrix multiply
510.3349 520.4258 ... 514.6938 517.7357
508.9028 ...... ... ...... 520.2448
...... ...... ... ...... ......
507.9561 ...... ... ...... 513.7372
521.5385 526.4379 ... 520.4982 516.9260
Custom Kernel Time: 2.797537 ms Performance: 6141.069320 GFLOPS

Testting the cublas implementation of matrix multiply
506.5235 519.2048 ... 503.0756 527.3256
513.7955 ...... ... ...... 526.7343
...... ...... ... ...... ......
502.1938 ...... ... ...... 514.4681
490.7425 508.1088 ... 486.1488 503.2191
cuBLAS Time: 2.810711 ms        Performance: 6112.286092 GFLOPS
(base) hongjie@kemove-ESC8000-G4:~/ml/cuda/MM$ make
nvcc compare.cu -lcublas -o main
(base) hongjie@kemove-ESC8000-G4:~/ml/cuda/MM$ ./main 2048 2048 2048 32

Testting the diy_cuda_global_mem implementation of matrix multiply
510.3349 520.4258 ... 514.6938 517.7357
508.9028 ...... ... ...... 520.2448
...... ...... ... ...... ......
507.9561 ...... ... ...... 513.7372
521.5385 526.4379 ... 520.4982 516.9260
Custom Kernel Time: 3.419901 ms Performance: 5023.499135 GFLOPS

Testting the diy_cuda_shared_mem implementation of matrix multiply
510.3349 520.4258 ... 514.6938 517.7357
508.9028 ...... ... ...... 520.2448
...... ...... ... ...... ......
507.9561 ...... ... ...... 513.7372
521.5385 526.4379 ... 520.4982 516.9260
Custom Kernel Time: 8.448617 ms Performance: 2033.453430 GFLOPS

Testting the cublas implementation of matrix multiply
506.5235 519.2048 ... 503.0756 527.3256
513.7955 ...... ... ...... 526.7343
...... ...... ... ...... ......
502.1938 ...... ... ...... 514.4681
490.7425 508.1088 ... 486.1488 503.2191
cuBLAS Time: 2.828008 ms        Performance: 6074.901333 GFLOPS
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	后来想想,确实更改后会更慢,因为本来的运行模型是32个线程作为一个warp来进行SIMT,然后如果是之前的代码</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
</pre></td><td class="rouge-code"><pre>temp += tile_a[threadIdx.y][j] * tile_b[j][threadIdx.x]
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	虽然在循环的时候涉及了列优先的访问,但是对于<strong>整个warp来说是行优先</strong>的访问(根据Idx.x)</p>

<h4 id="时间测试最后一起给出"><strong>时间测试：</strong>最后一起给出</h4>

<h3 id="任务2"><strong>任务2：</strong></h3>

<p>通过NVDIA的矩阵计算函数库CUBLAS计算矩阵相乘，矩阵规模从512增加至8192，并与任务1和任务2的矩阵乘法进行性能比较和分析，如果性能不如CUBLAS，思考并文字描述可能的改进方法（参考《计算机体系结构-量化研究方法》第四章）。</p>

<p>CUBLAS参考资料《CUBLAS_Library.pdf》，CUBLAS矩阵乘法参考第70页内容。</p>

<p>CUBLAS矩阵乘法例子，参考附件《matrixMulCUBLAS》</p>

<p>?	以下是比较矩阵乘法的表格，元素左边是计算时间ms，右边是浮点性能GFLOPS</p>

<h4 id="代码解释-1">代码解释：</h4>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1">//   m*k k*n = m*n</span>
	<span class="n">cublasH</span>     <span class="n">cublasHandle_t</span> <span class="n">handle</span>
    <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">)</span>
    <span class="c1">// Configure cuBLAS operations</span>
    <span class="n">cublasOperation_t</span> <span class="n">opA</span> <span class="o">=</span> <span class="n">CUBLAS_OP_N</span> <span class="c1">// No transpose for A</span>
    <span class="n">cublasOperation_t</span> <span class="n">opB</span> <span class="o">=</span> <span class="n">CUBLAS_OP_N</span> <span class="c1">// No transpose for B</span>
	<span class="n">cublasSgemm</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">opA</span><span class="p">,</span> <span class="n">opB</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">d_A</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1">// 我日，cublas 居然是列优先的</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	因为cublas使用列优先，所以输入为<strong>行优先分配的矩阵</strong>相当于先转置，BT*AT=CT， CT会列优先存到内存，再行优先就是C（这也太搞了）</p>

<h4 id="测试运行连同任务1">测试运行（连同任务1）</h4>

<p>?	包含三个算法，<strong>全局内存，共享内存，cublas</strong>,  测试了 blocksize从<strong>8~32</strong>， 矩阵维度从<strong>512到8192</strong></p>

<div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="nl">run</span><span class="o">:</span>
	<span class="p">@</span><span class="k">for </span>n <span class="k">in </span>8 16 24 32 <span class="k">do</span> <span class="se">\</span>
		<span class="k">for </span>size <span class="k">in </span>512 1024 2048 4096 8192 <span class="k">do</span> <span class="se">\</span>
			<span class="nb">echo</span> <span class="s2">"Running program with size </span><span class="nv">$$</span><span class="s2">size and </span><span class="nv">$$</span><span class="s2">n BLOCK"</span> | <span class="nb">tee</span> <span class="nt">-a</span> output.txt <span class="se">\</span>
			./<span class="p">$(</span>PROGRAM<span class="p">)</span> <span class="nv">$$</span>size <span class="nv">$$</span>size <span class="nv">$$</span>size <span class="nv">$$</span>n | <span class="nb">tee</span> <span class="nt">-a</span> output.txt <span class="se">\</span>
		<span class="k">done</span> <span class="se">\</span>
		<span class="nb">echo</span> <span class="s2">"End of program with </span><span class="nv">$$</span><span class="s2">n BLOCK SIZE"</span> | <span class="nb">tee</span> <span class="nt">-a</span> output.txt <span class="se">\</span>
		<span class="nb">echo</span> <span class="s2">""</span> | <span class="nb">tee</span> <span class="nt">-a</span> output.txt <span class="se">\</span>
	<span class="k">done</span> <span class="se">\</span>
	<span class="nb">echo</span> <span class="s2">"End of program"</span> | <span class="nb">tee</span> <span class="nt">-a</span> output.txt <span class="se">\</span>
	<span class="nb">echo</span> <span class="s2">""</span> | <span class="nb">tee</span> <span class="nt">-a</span> output.txt
</pre></td></tr></tbody></table></code></pre></div></div>

<p>使用的4090显卡数据如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
</pre></td><td class="rouge-code"><pre>Device 0: NVIDIA GeForce RTX 4090
  Clock Rate (GHz): 2.52 GHz
  CUDA Cores: 16384
  Theoretical Peak Performance (FP32): 82.5754 TFLOPS
Shared memory per block:  49152  bytes
</pre></td></tr></tbody></table></code></pre></div></div>

<h5 id="diy_cuda_global_mem">diy_cuda_global_mem</h5>

<table>
  <thead>
    <tr>
      <th>矩阵维度</th>
      <th>Blocksize:8*8</th>
      <th>Blocksize:16*16</th>
      <th>Blocksize:24*24</th>
      <th>Blocksize:32*32</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>0.1123 / 2389.79</td>
      <td>0.0872 / 3078.11</td>
      <td>0.0912 / 2944.79</td>
      <td><strong>0.0835 / 3216.22</strong></td>
    </tr>
    <tr>
      <td>1024</td>
      <td>0.5735 / 3744.41</td>
      <td>0.4539 / 4731.19</td>
      <td>0.4659 / 4609.26</td>
      <td><strong>0.4403 / 4877.43</strong></td>
    </tr>
    <tr>
      <td>2048</td>
      <td>4.2440 / 4048.06</td>
      <td>3.4547 / 4972.90</td>
      <td>3.4974 / 4912.13</td>
      <td><strong>3.4195 / 5024.12</strong></td>
    </tr>
    <tr>
      <td>4096</td>
      <td>39.1658 / 3509.16</td>
      <td><strong>27.0892 / 5073.57</strong></td>
      <td>27.4131 / 5013.62</td>
      <td>27.1107 / 5069.54</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>1079.691 / 1018.36</td>
      <td>297.402 / 3697.06</td>
      <td>247.026 / 4450.99</td>
      <td><strong>243.413 / 4517.06</strong></td>
    </tr>
  </tbody>
</table>

<h5 id="diy_cuda_shared_mem">diy_cuda_shared_mem</h5>

<table>
  <thead>
    <tr>
      <th>矩阵维度</th>
      <th>Blocksize:8*8</th>
      <th>Blocksize:16*16</th>
      <th>Blocksize:24*24</th>
      <th>Blocksize:32*32</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>0.0527 / 5091.72</td>
      <td><strong>0.0466 / 5764.87</strong></td>
      <td>0.0526 / 5104.11</td>
      <td>0.0522 / 5142.15</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>0.3210 / 6690.96</td>
      <td><strong>0.2939 / 7306.50</strong></td>
      <td>0.3231 / 6646.07</td>
      <td>0.3384 / 6346.85</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>2.4581 / 6989.17</td>
      <td><strong>2.3404 / 7340.62</strong></td>
      <td>2.4892 / 6901.76</td>
      <td>2.7261 / 6302.00</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>19.5173 / 7041.92</td>
      <td><strong>18.5687 / 7401.64</strong></td>
      <td>19.5364 / 7035.03</td>
      <td>23.4161 / 5869.42</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>149.465 / 7356.34</td>
      <td><strong>141.881 / 7749.54</strong></td>
      <td>158.358 / 6943.19</td>
      <td>188.046 / 5847.05</td>
    </tr>
  </tbody>
</table>

<h5 id="cublas">cublas</h5>

<table>
  <thead>
    <tr>
      <th>矩阵维度</th>
      <th>Blocksize:8*8</th>
      <th>Blocksize:16*16</th>
      <th>Blocksize:24*24</th>
      <th>Blocksize:32*32</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>0.1119 / 2399.42</td>
      <td>0.1038 / 2586.78</td>
      <td>0.1022 / 2626.80</td>
      <td>0.1051 / 2554.46</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>0.4111 / 5224.19</td>
      <td>0.3969 / 5411.12</td>
      <td>0.3960 / 5422.38</td>
      <td>0.3964 / 5418.05</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>2.8099 / 6114.08</td>
      <td>2.8116 / 6110.45</td>
      <td>2.8080 / 6118.18</td>
      <td>2.8110 / 6111.61</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>22.0671 / 6228.24</td>
      <td>22.0636 / 6229.22</td>
      <td>22.0639 / 6229.13</td>
      <td>22.0593 / 6230.42</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>169.151 / 6500.17</td>
      <td>169.021 / 6505.19</td>
      <td>169.004 / 6505.82</td>
      <td>169.397 / 6490.72</td>
    </tr>
  </tbody>
</table>

<h4 id="结果分析">结果分析：</h4>

<p>?	1. cublas库的运行不受blocksize的影响；</p>

<p>?	2. 对于普通的全局内存的函数，增大blocksize有助于提高运行速度；</p>

<p>?	3. 对于共享内存的版本，由于每个SM上的共享内存块是48KB,当blocksize增多到一定限度的时候（16*16），如果在增加线程数量，一个SM上的warps数量会继续增加，导致寄存器调度（切换warp）需要更多时间，同时在任务量不变得情况下，需要的SM数量也会下降，导致并行效率下降。</p>

<p>?	4. 对于规整的矩阵乘法，在部分任务上，共享内内存版本能超越cublas，但是也能看到cublas对于不同问题规模的运算的效率都保持在<strong>高水平</strong>。</p>

<p>?	5. 在cublas和共享内存的对种，只有32*32block_size的4096和8192乘法，共享内存函数是<strong>不如cublas的</strong>，在计算强度不变的情况下，根据<strong>屋顶线模型</strong>，考虑是<strong>内存访存</strong>的限制，在访问共享内存的时候出现了stall（<a href="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a>在kernel 3介绍了原因），所以可以<strong>考虑增加计算强度</strong>，在访存不变的情况下，同一个thread可以做更多的计算任务，之前是一个thread负责一个对应元素，现在可以负责一个小区域的元素（将结果矩阵分块）。</p>

<h3 id="任务3"><strong>任务3：</strong></h3>

<p>在信号处理、图像处理和其他工程/科学领域，卷积是一种使用广泛的技术。在深度学习领域，卷积神经网络(CNN)这种模型架构就得名于这种技术。在本实验中，我们将在GPU上实现卷积操作，注意这里的卷积是指神经网络中的卷积操作，与信号处理领域中的卷积操作不同，它不需要对Filter进行翻转，不考虑bias。</p>

<p>任务一通过CUDA实现直接卷积（滑窗法），输入从256增加至4096或者输入从32增加至512.</p>

<p>输入：Input和Kernel(3x3)</p>

<h4 id="问题描述">问题描述：</h4>

<p>用直接卷积的方式对Input进行卷积，这里只需要实现2D, height*width，通道channel(depth)设置为3，Kernel (Filter)大小设置为3*3，步幅(stride)分别设置为1，2，3，可能需要通过填充(padding)配合步幅(stride)完成CNN操作。注：实验的卷积操作不需要考虑bias(b)，bias设置为0.</p>

<p>输出：输出卷积结果以及计算时间</p>

<h4 id="代码解释-2">代码解释：</h4>

<p>?	以常用的张量描述（N, C, H,W）,其中<strong>N是batchsize，也就是张量数目， C是张量通道数，这里是RGB也就是3通道，  H是高度也就是矩阵的行数, W是宽度也就是张量的列数，</strong></p>

<p>?	首先需要对输入矩阵进行padding. paddedBlock是矩阵pdding之后的大小，（i+padding）遍历是为了0到padding-1行都是0， 然后（j+padding）是为了让左边的列padding为0.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="c1">// m n, depth分别是矩阵的行，列，通道数</span>
<span class="kt">float</span> <span class="o">*</span><span class="nf">padMatrix</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">matrix</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">padded_matrix</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">depth</span><span class="p">,</span> <span class="kt">int</span> <span class="n">padding</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">rows</span> <span class="o">=</span> <span class="n">m</span>
    <span class="kt">int</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">n</span>
    <span class="kt">int</span> <span class="n">paddedCols</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>
    <span class="kt">int</span> <span class="n">paddedRows</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>
    <span class="kt">int</span> <span class="n">paddedBlcok</span> <span class="o">=</span> <span class="n">paddedRows</span> <span class="o">*</span> <span class="n">paddedCols</span>
    <span class="kt">int</span> <span class="n">originblock</span> <span class="o">=</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">depth</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">rows</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">cols</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="n">padded_matrix</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">paddedBlcok</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">padding</span><span class="p">)</span> <span class="o">*</span> <span class="n">paddedCols</span> <span class="o">+</span> <span class="n">j</span> <span class="o">+</span> <span class="n">padding</span><span class="p">]</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">originblock</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">padded_matrix</span>
<span class="p">}</span>   
   <span class="n">memset</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">paddedRows</span> <span class="o">*</span> <span class="n">paddedCols</span> <span class="o">*</span> <span class="n">input</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">)</span>  <span class="c1">//需要把原来的内存设置为0，否则padding的时候可能会出错</span>
    <span class="n">padMatrix</span><span class="p">(</span><span class="n">h_input</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>然后是滑窗卷积函数：</p>

<p><img src="https://img-blog.csdnimg.cn/b7fc56d8c1634f0aadc00855206451e7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_19,color_FFFFFF,t_70,g_se,x_16" alt="[人工智能-深度学习-27]：卷积神经网络CNN - 核心概念（卷积、滑动、填充、参数共享、通道）_cnn 本地连接,参数共享-CSDN博客" /></p>

<ul>
  <li>(row % stride == 0 &amp;&amp; col % stride == 0是为了步长滑动，相当于滑动卷积窗口</li>
  <li>row &lt; paddedm - kernelSet.rows + 1 &amp;&amp; col &lt; paddedn - kernelSet.cols + 1是为了卷积窗口在边界的时候不会越界（避免滑出矩阵边界）</li>
  <li>然后就是三次遍历，分别是通道，行，列，将卷积核和对应的局部矩阵作向量点积，最后化为一个数字，temp</li>
  <li>因为是每一个thread负责一个对应输出的元素，所以不存在共享变量读取竞争的问题</li>
</ul>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre><span class="c1">//输入m,n是原本的行列数</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">conv2d_cal</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">output_block</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="k">const</span> <span class="n">Kernel</span> <span class="n">kernelSet</span><span class="p">,</span> <span class="kt">int</span> <span class="n">stride</span><span class="p">,</span> <span class="kt">int</span> <span class="n">padding</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
    <span class="kt">int</span> <span class="n">paddedm</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>
    <span class="kt">int</span> <span class="n">paddedn</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>

    <span class="kt">int</span> <span class="n">conved_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1">//计算卷积后的列数</span>
    
    <span class="c1">// extern __shared__ float temp[] // 使用动态共享内存，适配 kernelSet.numKernels 注意会出现竞争！！！</span>
    <span class="kt">float</span> <span class="n">temp</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">%</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">%</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">paddedm</span> <span class="o">-</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">paddedn</span> <span class="o">-</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">numKernels</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
                <span class="p">{</span>
                    <span class="kt">int</span> <span class="n">input_row</span> <span class="o">=</span> <span class="n">row</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="kt">int</span> <span class="n">input_col</span> <span class="o">=</span> <span class="n">col</span> <span class="o">+</span> <span class="n">j</span>
                    <span class="kt">int</span> <span class="n">kernel_idx</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">j</span>
                    <span class="n">temp</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">input_row</span> <span class="o">*</span> <span class="n">paddedn</span> <span class="o">+</span> <span class="n">input_col</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">deviceKernels</span><span class="p">[</span><span class="n">kernel_idx</span><span class="p">]</span>
                <span class="p">}</span>
            <span class="p">}</span>
            <span class="c1">// 写入输出           </span>
        <span class="p">}</span>
        <span class="n">output</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">conved_n</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="测试运行">测试运行：</h4>

<h5 id="正确性">正确性</h5>

<p>对于卷积任务（1，3，5，5）（1，3，3，3）stride =x padding =1验证</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="n">Kernel</span> <span class="mi">0</span> <span class="n">values</span><span class="o">:</span>  	  
<span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> 
<span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> 
<span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span> 
<span class="n">Kernel</span> <span class="mi">1</span> <span class="n">values</span><span class="o">:</span>  	  
<span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> 
<span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> 
<span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span>
<span class="n">Kernel</span> <span class="mi">2</span> <span class="n">values</span><span class="o">:</span>  	  
<span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> 
<span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> 
<span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span>
<span class="n">atfer</span> <span class="n">padding</span><span class="o">:</span>
<span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">1.000000</span> <span class="mf">2.000000</span> <span class="mf">3.000000</span> <span class="mf">4.000000</span> <span class="mf">5.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">6.000000</span> <span class="mf">7.000000</span> <span class="mf">8.000000</span> <span class="mf">9.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">1.000000</span> <span class="mf">2.000000</span> <span class="mf">3.000000</span> <span class="mf">4.000000</span> <span class="mf">5.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">6.000000</span> <span class="mf">7.000000</span> <span class="mf">8.000000</span> <span class="mf">9.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">1.000000</span> <span class="mf">2.000000</span> <span class="mf">3.000000</span> <span class="mf">4.000000</span> <span class="mf">5.000000</span> <span class="mf">0.000000</span> 
<span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> <span class="mf">0.000000</span> 
</pre></td></tr></tbody></table></code></pre></div></div>

<p>验证stride =1的正确性，打印输出矩阵(0,2)元素的一个通道的计算过程，可以看到shape正确，计算过程也正确(3个通道要乘3)</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="nl">row:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">1.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">2.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">2.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">8.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">5.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">23.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">6.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">47.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">7.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">7.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">96.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">160.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">241.000000</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">sliding</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">4.198948</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">606.000000</span> <span class="mf">723.000000</span> <span class="mf">570.000000</span> <span class="mf">312.000000</span> 
<span class="mf">318.000000</span> <span class="mf">513.000000</span> <span class="mf">648.000000</span> <span class="mf">603.000000</span> <span class="mf">354.000000</span> 
<span class="mf">483.000000</span> <span class="mf">738.000000</span> <span class="mf">873.000000</span> <span class="mf">648.000000</span> <span class="mf">339.000000</span> 
<span class="mf">318.000000</span> <span class="mf">513.000000</span> <span class="mf">648.000000</span> <span class="mf">603.000000</span> <span class="mf">354.000000</span> 
<span class="mf">150.000000</span> <span class="mf">228.000000</span> <span class="mf">291.000000</span> <span class="mf">264.000000</span> <span class="mf">150.000000</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	验证stride =2的正确性,打印出输出矩阵的(0,1)元素一个通道的计算过程，可以看到，shape是正确的</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="nl">row:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">1.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">2.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">2.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">8.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">5.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">23.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">6.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">47.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">7.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">7.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">96.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">160.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">241.000000</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">sliding</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">4.179901</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">723.000000</span> <span class="mf">312.000000</span> 
<span class="mf">483.000000</span> <span class="mf">873.000000</span> <span class="mf">339.000000</span> 
<span class="mf">150.000000</span> <span class="mf">291.000000</span> <span class="mf">150.000000</span> 
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	验证stride =3的正确性,打印出输出矩阵的(0,1)元素一个通道的计算过程，可以看到，shape是正确的 结果也正确</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="nl">row:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">1.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">2.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">0.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">3.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">12.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">4.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">5.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">32.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">5.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">6.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">62.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">0</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">7.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">118.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">8.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">190.000000</span>
<span class="n">row</span><span class="o">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">:</span><span class="mi">2</span><span class="p">,</span> <span class="n">input</span><span class="o">:</span><span class="mf">0.000000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">:</span><span class="mf">9.000000</span><span class="p">,</span> <span class="n">temp</span><span class="o">:</span> <span class="mf">190.000000</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">sliding</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">4.153343</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">570.000000</span> 
<span class="mf">318.000000</span> <span class="mf">603.000000</span> 

</pre></td></tr></tbody></table></code></pre></div></div>

<h5 id="时间测试">时间测试</h5>

<p>计算复杂度：O(Hout?Wout?Cout?Cin?K2)</p>

<table>
  <thead>
    <tr>
      <th>Matrix Size</th>
      <th>Stride 1 Time (ms)</th>
      <th>Stride 2 Time (ms)</th>
      <th>Stride 3 Time (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>7.709874</td>
      <td>7.900139</td>
      <td>9.811203</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>26.421349</td>
      <td>34.015903</td>
      <td>30.129812</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>104.506943</td>
      <td>108.413010</td>
      <td>105.430489</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>444.949066</td>
      <td>417.461884</td>
      <td>434.954102</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>1763.79</td>
      <td>1575.86</td>
      <td>1716.98</td>
    </tr>
  </tbody>
</table>

<h3 id="任务4"><strong>任务4：</strong></h3>

<p>使用im2col方法结合任务1实现的GEMM（通用矩阵乘法）实现卷积操作。输入从256增加至4096或者输入从32增加至512，具体实现的过程可以参考下面的图片和参考资料。</p>

<p>输入：Input和Kernel (Filter)</p>

<h4 id="问题描述-1">问题描述：</h4>

<p>用im2col的方式对Input进行卷积，这里只需要实现2D, height*width，通道channel(depth)设置为3，Kernel (Filter)大小设置为3*3。 注：实验的卷积操作不需要考虑bias(b)，bias设置为0，步幅(stride)分别设置为1，2，3。</p>

<p>输出：卷积结果和时间。</p>

<p><img src="C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20241226203602867.png" alt="image-20241226203602867" /></p>

<h4 id="代码解释-3">代码解释：</h4>

<p>?	首先还是先补充padding,同上省略</p>

<p>?	然后是最麻烦的，将张量展开为矩阵</p>

<p>先计算出展开后的形状，高度（行数）是卷积核的大小 filter_size， 宽度是列数，是卷积后矩阵的大小<strong>converted_n</strong></p>

<p>然后五层展开，第一二层移动卷积窗口，里面三层是复制原矩阵的内容到展开后的矩阵</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="rouge-code"><pre>    <span class="kt">int</span> <span class="n">padded_m</span> <span class="o">=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">rows</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>  <span class="c1">//padding</span>
    <span class="kt">int</span> <span class="n">padded_n</span> <span class="o">=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span>
    <span class="kt">int</span> <span class="n">convable_m</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">rows</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1">//after convolution</span>
    <span class="kt">int</span> <span class="n">convable_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="kt">int</span> <span class="n">converted_n</span> <span class="o">=</span> <span class="n">convable_m</span> <span class="o">*</span> <span class="n">convable_n</span>  <span class="c1">// converted to a flat matrix</span>
    <span class="kt">int</span> <span class="n">converted_m</span> <span class="o">=</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span>

    <span class="kt">int</span> <span class="n">block</span> <span class="o">=</span> <span class="n">converted_m</span> <span class="o">*</span> <span class="n">converted_n</span>
    <span class="kt">int</span> <span class="n">padded_block</span> <span class="o">=</span> <span class="n">padded_m</span> <span class="o">*</span> <span class="n">padded_n</span>

    <span class="kt">float</span> <span class="o">*</span><span class="n">temp</span>
    <span class="c1">// printf("hello\n")</span>
    <span class="n">cudaMallocHost</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">temp</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">padded_block</span> <span class="o">*</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">)</span>  <span class="c1">//create a temp to get the padded matrix in host mem</span>

    <span class="n">padMatrix</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">h_input</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="n">h_input</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span> <span class="n">h_input</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>


    <span class="kt">float</span> <span class="o">*</span><span class="n">h_temp</span>
    <span class="nf">cudaMallocHost</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">h_temp</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span> <span class="o">*</span> <span class="n">block</span><span class="p">)</span>  <span class="c1">//for converted matrix</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">+</span><span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span><span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">padded_m</span> <span class="n">i</span><span class="o">+=</span><span class="n">stride</span><span class="p">)</span> <span class="c1">// the next 2 iterations are for sliding the conv window</span>
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="n">j</span><span class="o">+</span><span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span><span class="o">-</span><span class="mi">1</span><span class="o">&lt;</span><span class="n">padded_n</span> <span class="n">j</span><span class="o">+=</span><span class="n">stride</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="c1">// the naxt 2 iterations are for inside the conv window</span>
            <span class="p">{</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span>
                <span class="p">{</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="c1">// the k iteration for the input matrixs(could be more than 1)</span>
                    <span class="p">{</span>
                        <span class="kt">int</span> <span class="n">li</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">block</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">converted_n</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span> <span class="o">*</span> <span class="n">convable_n</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span><span class="o">/</span><span class="n">stride</span><span class="p">)</span>
                        <span class="kt">int</span> <span class="n">ri</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">padded_block</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">padded_m</span> <span class="o">+</span> <span class="n">j</span> <span class="o">+</span> <span class="n">b</span>
                        <span class="n">h_temp</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?</p>

<p>?	然后是卷积核的展开，然后直接调用之前实现的矩阵乘算法，就可以实现结果的计算.</p>

<p>?	Img2col展开后的矩阵乘法是不规整的,对于普通实现的矩阵乘法压力很大,所以我在矩阵乘法阶段直接使用了<strong>cublas的矩阵乘法库函数</strong></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">gpu_gemm_cublas</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_result</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">//   m*n x n*k =m *k</span>
    <span class="n">cublasHandle_t</span> <span class="n">handle</span>
    <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">)</span>
    <span class="c1">// Configure cuBLAS operations</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">cublasOperation_t</span> <span class="n">opA</span> <span class="o">=</span> <span class="n">CUBLAS_OP_N</span>                                           <span class="c1">// No transpose for A</span>
    <span class="n">cublasOperation_t</span> <span class="n">opB</span> <span class="o">=</span> <span class="n">CUBLAS_OP_N</span>                                           <span class="c1">// No transpose for B</span>
    <span class="n">cublasSgemm</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">opA</span><span class="p">,</span> <span class="n">opB</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d_a</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span> <span class="n">d_result</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="c1">// 我日，cublas 居然是列优先的</span>
    <span class="c1">// cudaMemcpy(h_output.h_input, d_result, sizeof(float)*converted_n, cudaMemcpyDeviceToHost)</span>
<span class="p">}</span>

</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	另外在实验中发现,程序的大量时间都用在了展开矩阵上,后续优化可以考虑将展开写为核函数.</p>

<h4 id="问题调试">问题调试</h4>

<p>在实现这个Img2col算法的时候，出现了奇怪的问题，核函数只能算出32个数字，根据对核函数，也就是之前实现的矩阵乘法函数的分析：</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_matrix_mult_gm</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
    <span class="kt">float</span> <span class="n">temp</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">)</span> <span class="c1">// Ensure bounds are within the matrix dimensions</span>
    <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">temp</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="n">c</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"%d %d index:%d, reuslt: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">row</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">col</span> <span class="p">,</span><span class="n">temp</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>以及切分矩阵的语句：</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
</pre></td><td class="rouge-code"><pre>    <span class="n">dim3</span> <span class="n">blockDim</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span> 
    <span class="n">dim3</span> <span class="n">gridDim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">//这里我以为1024的线程足够计算简单任务，没有设置</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>?	可以发现对于<strong>瘦高的</strong>矩阵乘法，之前对方块矩阵的切分方式可能导致有的元素没有参加运算，比如（1， 3 ， 1024 ，1024）的图像矩阵和(1, 3, 3, 3 )卷积核的卷积， 等价(1, 27)的卷积核向量和（27， 1024*1024）的矩阵相乘，之前的矩阵分割算法，是对右边矩阵的列和左边矩阵的行分别进行分割，然后对应相乘计算出结果，这里行是1，列是1024x1024，如果使用32，32的block shape，就会有行方向31组线程浪费， 列方向又远远不足以计算任务。</p>

<p>解决的办法：重新设置切割语句，适应不同矩阵乘法的需求</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
</pre></td><td class="rouge-code"><pre>    <span class="kt">int</span> <span class="n">blockx</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">blocky</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">//和warp对应，还是需要blocksize是32的倍数， 但这个时候，设置为矮宽的形状</span>
    <span class="n">dim3</span> <span class="n">blockDim</span><span class="p">(</span><span class="n">blockx</span><span class="p">,</span> <span class="n">blocky</span><span class="p">)</span>
    <span class="kt">int</span> <span class="n">gridx</span> <span class="o">=</span> <span class="p">(</span><span class="n">converted_n</span> <span class="o">+</span> <span class="n">blockx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockx</span>  <span class="c1">//计算网格</span>
    <span class="kt">int</span> <span class="n">gridy</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">//由于左边向量的高度还是1，所以考虑块的排布也都是扁平的</span>
    <span class="n">dim3</span> <span class="n">gridDim</span><span class="p">(</span><span class="n">gridx</span><span class="p">,</span> <span class="n">gridy</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>这个时候出现了另外一个问题，就是使用滑动窗口算法，和使用img2col算法（共享内存版本），他们都只能算矩阵维度最大是1022的，由于1022+2=1024是32的倍数，这个时候我考虑是blocksize大小的问题，我还需要重新检查这两个算法，确定他们的切分方式和blocksize的关系，同时共享内存之前的写法也是和blocksize关联的，但是内存是有限的，不能让blocksize太大，所以需要进一步检查。</p>

<p>?	好吧，原来是我错误的将block和grid写反了，而4090gpu的threadperblock的限制是1024，当维度一大，将grid和block搞反就以为着会超过1024的限制，导致报错。</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
</pre></td><td class="rouge-code"><pre><span class="o">&lt;&lt;&lt;</span><span class="n">BLock</span><span class="p">,</span> <span class="n">Grid</span><span class="o">&gt;&gt;</span> <span class="n">kernel_function</span><span class="p">()</span><span class="c1">//我在这里搞反了顺序,Grid应该在前面</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="测试运行-1">测试运行：</h4>

<h5 id="正确性-1">正确性:</h5>

<p>和上面的任务使用同样的输入确定正确性</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="n">stride</span> <span class="o">:</span><span class="mi">1</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">img2col</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">450.193542</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">606.000000</span> <span class="mf">723.000000</span> <span class="mf">570.000000</span> <span class="mf">312.000000</span> 
<span class="mf">318.000000</span> <span class="mf">513.000000</span> <span class="mf">648.000000</span> <span class="mf">603.000000</span> <span class="mf">354.000000</span> 
<span class="mf">483.000000</span> <span class="mf">738.000000</span> <span class="mf">873.000000</span> <span class="mf">648.000000</span> <span class="mf">339.000000</span> 
<span class="mf">318.000000</span> <span class="mf">513.000000</span> <span class="mf">648.000000</span> <span class="mf">603.000000</span> <span class="mf">354.000000</span> 
<span class="mf">150.000000</span> <span class="mf">228.000000</span> <span class="mf">291.000000</span> <span class="mf">264.000000</span> <span class="mf">150.000000</span>
<span class="n">stride</span><span class="o">:</span><span class="mi">2</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">img2col</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">446.803680</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">723.000000</span> <span class="mf">312.000000</span> 
<span class="mf">483.000000</span> <span class="mf">873.000000</span> <span class="mf">339.000000</span> 
<span class="mf">150.000000</span> <span class="mf">291.000000</span> <span class="mf">150.000000</span> 
<span class="n">stride</span><span class="o">:</span><span class="mi">3</span>
<span class="n">Custom</span> <span class="n">Kernel</span> <span class="n">Time</span> <span class="k">for</span> <span class="n">img2col</span> <span class="n">conv</span><span class="o">:</span> <span class="mf">424.988342</span> <span class="n">ms</span>
<span class="n">Printing</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">printing</span> <span class="mi">0</span><span class="n">th</span> <span class="n">matrix</span>
<span class="mf">384.000000</span> <span class="mf">570.000000</span> 
<span class="mf">318.000000</span> <span class="mf">603.000000</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h5 id="时间测试-1">时间测试</h5>

<p><strong>Img2Col Convolution 时间统计表</strong></p>

<table>
  <thead>
    <tr>
      <th>Matrix Size</th>
      <th>Stride 1 Time (ms)</th>
      <th>Stride 2 Time (ms)</th>
      <th>Stride 3 Time (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>2971.511719</td>
      <td>1231.348145</td>
      <td>1313.612671</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>975.632629</td>
      <td>688.659485</td>
      <td>920.661926</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>6211.638184</td>
      <td>2188.381104</td>
      <td>2943.849365</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>24702.511719</td>
      <td>10237.832031</td>
      <td>13833.249023</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<h3 id="任务5"><strong>任务5：</strong></h3>

<h4 id="问题描述-2">问题描述</h4>

<p>NVIDIA cuDNN是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。</p>

<p>使用cuDNN提供的卷积方法进行卷积操作，记录其相应Input的卷积时间，与自己实现的卷积操作进行比较。如果性能不如cuDNN，用文字描述可能的改进方法。</p>

<p><strong>CNN参考资料，见实验发布网站</strong></p>

<p>斯坦福人工智能课件Convolutional Neural Networks，by Fei-Fei Li &amp; Andrej Karpathy &amp; Justin Johnson</p>

<p><strong>其他参考资料 （搜索以下关键词）</strong></p>

<p>[1]如何理解卷积神经网络（CNN）中的卷积和池化</p>

<p>[2] Convolutional Neural Networks (CNNs / ConvNets) https://cs231n.github.io/convolutional-networks/</p>

<p>[3]im2col的原理和实现</p>

<p>[4]cuDNN安装教程</p>

<p>[5] convolutional-neural-networks</p>

<p>https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks</p>

<h4 id="代码解释-4">代码解释:</h4>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">conv_cudnn</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">h_input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">h_output</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">kernelSet</span><span class="p">,</span> <span class="kt">int</span> <span class="n">rows</span> <span class="p">,</span><span class="kt">int</span> <span class="n">cols</span><span class="p">,</span> <span class="kt">int</span> <span class="n">channels</span><span class="p">,</span> <span class="kt">int</span> <span class="n">kernel_row</span><span class="p">,</span> <span class="kt">int</span> <span class="n">kernel_col</span><span class="p">,</span> <span class="kt">int</span> <span class="n">stride</span><span class="p">,</span> <span class="kt">int</span> <span class="n">padding</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">// Batch size</span>
    <span class="kt">int</span> <span class="n">C</span> <span class="o">=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span>
    <span class="kt">int</span> <span class="n">H</span> <span class="o">=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">rows</span>
    <span class="kt">int</span> <span class="n">W</span> <span class="o">=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">cols</span>

    <span class="kt">int</span> <span class="n">R</span> <span class="o">=</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">rows</span>
    <span class="kt">int</span> <span class="n">S</span> <span class="o">=</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">cols</span>

    <span class="kt">int</span> <span class="n">conved_H</span> <span class="o">=</span> <span class="n">h_output</span><span class="p">.</span><span class="n">rows</span>
    <span class="kt">int</span> <span class="n">conved_W</span> <span class="o">=</span> <span class="n">h_output</span><span class="p">.</span><span class="n">cols</span>

    <span class="c1">// Check if the kernel count matches the input channels</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">kernelSet</span><span class="p">.</span><span class="n">numKernels</span> <span class="o">!=</span> <span class="n">h_input</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Err: not paired kernels and input</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="c1">// Check if the output dimensions match the expected sizes</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">H</span> <span class="o">-</span> <span class="n">R</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">!=</span> <span class="n">conved_H</span> <span class="o">||</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">S</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">!=</span> <span class="n">conved_W</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Err: not paired conv src and result space</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="n">cudnnHandle_t</span> <span class="n">cudnn</span>
    <span class="nf">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cudnn</span><span class="p">))</span>

    <span class="c1">// Allocate device memory for input, output, and kernels</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_input</span><span class="p">,</span> <span class="o">*</span><span class="n">d_output</span><span class="p">,</span> <span class="o">*</span><span class="n">d_kernels</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_input</span><span class="p">,</span> <span class="n">C</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">))</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_output</span><span class="p">,</span> <span class="n">h_output</span><span class="p">.</span><span class="n">matrix_nums</span> <span class="o">*</span> <span class="n">conved_H</span> <span class="o">*</span> <span class="n">conved_W</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">))</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_kernels</span><span class="p">,</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">numKernels</span> <span class="o">*</span> <span class="n">C</span> <span class="o">*</span> <span class="n">R</span> <span class="o">*</span> <span class="n">S</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">))</span>

    <span class="c1">// Copy input and kernel data from host to device</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">h_input</span><span class="p">.</span><span class="n">h_input</span><span class="p">,</span> <span class="n">C</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">)</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_kernels</span><span class="p">,</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">deviceKernels</span><span class="p">,</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">numKernels</span> <span class="o">*</span> <span class="n">C</span> <span class="o">*</span> <span class="n">R</span> <span class="o">*</span> <span class="n">S</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">)</span>

    <span class="n">cudnnTensorDescriptor_t</span> <span class="n">input_desc</span><span class="p">,</span> <span class="n">output_desc</span>
    <span class="n">cudnnFilterDescriptor_t</span> <span class="n">kernel_desc</span>
    <span class="n">cudnnConvolutionDescriptor_t</span> <span class="n">conv_desc</span>

    <span class="c1">// Create cuDNN descriptors</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnCreateTensorDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input_desc</span><span class="p">))</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnCreateTensorDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output_desc</span><span class="p">))</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnCreateFilterDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel_desc</span><span class="p">))</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnCreateConvolutionDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">conv_desc</span><span class="p">))</span>

    <span class="c1">// Set input tensor descriptor (NCHW format)</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnSetTensor4dDescriptor</span><span class="p">(</span><span class="n">input_desc</span><span class="p">,</span> <span class="n">CUDNN_TENSOR_NCHW</span><span class="p">,</span> <span class="n">CUDNN_DATA_FLOAT</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>

    <span class="c1">// Set output tensor descriptor (NCHW format)</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnSetTensor4dDescriptor</span><span class="p">(</span><span class="n">output_desc</span><span class="p">,</span> <span class="n">CUDNN_TENSOR_NCHW</span><span class="p">,</span> <span class="n">CUDNN_DATA_FLOAT</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">h_output</span><span class="p">.</span><span class="n">matrix_nums</span><span class="p">,</span> <span class="n">conved_H</span><span class="p">,</span> <span class="n">conved_W</span><span class="p">))</span>

    <span class="c1">// Set kernel descriptor (filter)</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnSetFilter4dDescriptor</span><span class="p">(</span><span class="n">kernel_desc</span><span class="p">,</span> <span class="n">CUDNN_DATA_FLOAT</span><span class="p">,</span> <span class="n">CUDNN_TENSOR_NCHW</span><span class="p">,</span> <span class="n">kernelSet</span><span class="p">.</span><span class="n">numKernels</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">S</span><span class="p">))</span>

    <span class="c1">// Set convolution descriptor</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnSetConvolution2dDescriptor</span><span class="p">(</span><span class="n">conv_desc</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CUDNN_CROSS_CORRELATION</span><span class="p">,</span> <span class="n">CUDNN_DATA_FLOAT</span><span class="p">))</span>

    <span class="c1">// Get the output dimensions from cuDNN</span>
    <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnGetConvolution2dForwardOutputDim</span><span class="p">(</span><span class="n">conv_desc</span><span class="p">,</span> <span class="n">input_desc</span><span class="p">,</span> <span class="n">kernel_desc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">n</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">c</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">h</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">w</span><span class="p">))</span>

    <span class="c1">// Check the computed dimensions</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">!=</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">c</span> <span class="o">!=</span> <span class="n">h_output</span><span class="p">.</span><span class="n">matrix_nums</span> <span class="o">||</span> <span class="n">h</span> <span class="o">!=</span> <span class="n">conved_H</span> <span class="o">||</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">conved_W</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Err: computed output dimensions don't match expected ones. Got (%d, %d, %d, %d)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>

    <span class="c1">// Set the output tensor descriptor with the computed dimensions</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnSetTensor4dDescriptor</span><span class="p">(</span><span class="n">output_desc</span><span class="p">,</span> <span class="n">CUDNN_TENSOR_NCHW</span><span class="p">,</span> <span class="n">CUDNN_DATA_FLOAT</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>

    <span class="c1">// Perform the convolution</span>
    <span class="kt">float</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="n">f</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span>
    <span class="n">CHECK_CUDNN_ERR</span><span class="p">(</span><span class="n">cudnnConvolutionForward</span><span class="p">(</span><span class="n">cudnn</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">alpha</span><span class="p">,</span> <span class="n">input_desc</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">kernel_desc</span><span class="p">,</span> <span class="n">d_kernels</span><span class="p">,</span> <span class="n">conv_desc</span><span class="p">,</span> <span class="n">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">beta</span><span class="p">,</span> <span class="n">output_desc</span><span class="p">,</span> <span class="n">d_output</span><span class="p">))</span>

    <span class="c1">// Copy the result back to host memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_output</span><span class="p">.,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">h_output</span><span class="p">.</span><span class="n">matrix_nums</span> <span class="o">*</span> <span class="n">conved_H</span> <span class="o">*</span> <span class="n">conved_W</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">)</span>

    <span class="c1">// Free device memory and cuDNN descriptors</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_input</span><span class="p">)</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_output</span><span class="p">)</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_kernels</span><span class="p">)</span>
    <span class="n">cudnnDestroyTensorDescriptor</span><span class="p">(</span><span class="n">input_desc</span><span class="p">)</span>
    <span class="n">cudnnDestroyTensorDescriptor</span><span class="p">(</span><span class="n">output_desc</span><span class="p">)</span>
    <span class="n">cudnnDestroyFilterDescriptor</span><span class="p">(</span><span class="n">kernel_desc</span><span class="p">)</span>
    <span class="n">cudnnDestroyConvolutionDescriptor</span><span class="p">(</span><span class="n">conv_desc</span><span class="p">)</span>
    <span class="n">cudnnDestroy</span><span class="p">(</span><span class="n">cudnn</span><span class="p">)</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="测试运行-2">测试运行:</h4>

<p><img src="C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20241227144804690.png" alt="image-20241227144804690" /></p>

<p><img src="C:\Users\rogers\AppData\Roaming\Typora\typora-user-images\image-20241227144841510.png" alt="image-20241227144841510" /></p>

<h5 id="时间测试-2">时间测试:</h5>

<p>Kernel Performance Statistics</p>

<table>
  <thead>
    <tr>
      <th>Matrix Size</th>
      <th>Stride 1 Time (ms)</th>
      <th>Stride 2 Time (ms)</th>
      <th>Stride 3 Time (ms)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>512</td>
      <td>826.092407</td>
      <td>811.012817</td>
      <td>739.268188</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>917.171265</td>
      <td>731.723877</td>
      <td>780.229431</td>
    </tr>
    <tr>
      <td>2048</td>
      <td>872.473267</td>
      <td>805.256348</td>
      <td>774.350159</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>1059.660156</td>
      <td>972.454163</td>
      <td>950.795410</td>
    </tr>
    <tr>
      <td>8192</td>
      <td>4997.216797</td>
      <td>1728.338379</td>
      <td>1832.116211</td>
    </tr>
  </tbody>
</table>

<h4 id="结果分析-1">结果分析</h4>

<p>?	使用滑窗卷积法在大部分任务上和cudnn的计算时间在同一个数量级,甚至计算时间更少</p>

<p>?	使用Img2col算法,由于在展开的时候使用的是cpu展开,对于大型矩阵是非常慢的,后续考虑将展开函数<strong>分配到核函数进行</strong>, 然后生成的矩阵使用<strong>分块矩阵乘法加速</strong>计算,比如这里是1x27和27x4096的矩阵计算,可以对4096列分块计算,每一个线程负责一个列的计算.</p>


                    
                    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img
                            alt="Creative Commons License"
                            style="border-width:0;max-width: 40%;margin-left: 0px;margin-bottom: 5px;"
                            src="/img/icons/cc_byncsa.flat.guokr.svg" /></a><text style="font-size: 14px">本作品采用<a
                            rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享
                            4.0
                            国际许可协议</a>进行许可。<br />This work is licensed under a <a rel="license"
                            href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons
                            Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</text><br />
                    

                    <hr style="visibility: hidden;">
                    <ul class="pager">
                        
                        <li class="previous">
                            <a href="/2024-11-13-HPClab4/"
                                data-toggle="tooltip" data-placement="top" title="HPC lab4 一个传热问题的并行计算">
                                Previous<br>
                                <span>HPC lab4 一个传热问题的并行计算</span>
                            </a>
                        </li>
                        
                        
                        <li class="next">
                            <a href="/2024-12-26-Diary/"
                                data-toggle="tooltip" data-placement="top" title="并行加速比">
                                Next<br>
                                <span>并行加速比</span>
                            </a>
                        </li>
                        
                    </ul>
                    <hr style="visibility: hidden;">

                    <!--Gitalk评论start  -->
                    
                    <!-- Gitalk end -->

                    

                    
                </div>

                <!-- Side Catalog Container -->
                
                <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                    <div class="side-catalog">
                        <hr class="hidden-sm hidden-xs">
                        <h5>
                            <a class="catalog-toggle" href="#">CATALOG</a>
                        </h5>
                        <ul class="catalog-body"></ul>
                    </div>
                </div>
                

                <!-- Sidebar Container -->
                <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">
                    <!-- DONE: FRIEND has a link to the last tag -->

                    <!-- Featured Tags -->
                    


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        
        
        
                <a data-sort="0009" 
                    href="/archive/?tag=OS"
                    title="OS"
                    rel="7">OS</a>
        
                <a data-sort="0009" 
                    href="/archive/?tag=HPC"
                    title="HPC"
                    rel="7">HPC</a>
    </div>
</section>


                    <!-- Friends Blog -->
                    
                </div>
            </div>
        </div>
</article>

<!-- add support for mathjax by voleking-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    SVG: {
      scale: 90
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'], ['\\[','\\]'], ['\\(','\\)'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


<!-- CleverYh -->

<!-- End CleverYh -->








<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js", function () {
        anchors.options = {
            visible: 'hover',
            placement: 'right',
            // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link {
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top: -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">
    

        
        
        
        
        <li>
            <a target="_blank" title="Github" href="https://github.com/luohj29">
                <!-- <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span> -->
                <span class="fa-stack fa-lg">
                    <i class="fa-brands fa-github-alt fa-stack-1x"></i>
                </span>
            </a>
        </li>
        
        
        
        
        
        
        
        
    </ul>

                <p class="copyright text-muted">
                    <text style="opacity: 1; color: #007799;">&copy; 2019 - 2025 <a
                            href="https://luohj29.github.io/">RogersLuo's Blog</a>
                    </text><br>
                    <span style="opacity: 0.78; color: #007799;font-family: 'CamingoCodeRegular';">
                        <!-- Default Statcounter code for lzzmm
                    https://lzzmm.github.io -->
                        <script type="text/javascript">
                            var sc_project = 12680417;
                            var sc_invisible = 0;
                            var sc_security = "74b5722d";
                            var sc_text = 2;
                            var scJsHost = "https://";
                            document.write("<script type='text/javascript' src='" +
                                scJsHost +
                                "statcounter.com/counter/counter.js'></" + "script>");
                        </script>
                        <!-- <noscript>
                        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                                    class="statcounter" src="https://c.statcounter.com/12680417/0/74b5722d/0/" alt="Web Analytics"
                                    referrerPolicy="no-referrer-when-downgrade"></a></div>
                        </noscript> -->
                        <!-- End of Statcounter Code -->
                        <span> visits | </span>
                        <span>Uptime:</span><span id="display_live_time"></span>
                        <script>function blog_live_time() {
                                window.setTimeout(blog_live_time, 1000);
                                const start = new Date('2020-02-16T14:37:00');
                                const now = new Date();
                                const timeDiff = (now.getTime() - start.getTime());
                                const msPerMinute = 60 * 1000;
                                const msPerHour = 60 * msPerMinute;
                                const msPerDay = 24 * msPerHour;
                                const passDay = Math.floor(timeDiff / msPerDay);
                                const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
                                const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
                                const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
                                display_live_time.innerHTML = " " + passDay + "d " + passHour + "h " + passMinute + "m " + passSecond + "s ";
                            }
                            blog_live_time();
                        </script>
                    </span><br>
                    <text style="opacity: 0.66; color: #0099bb;">Powered by <a
                            href="https://github.com/luohj29/luohj29.github.io"
                            target="_blank">luohj29.github.io</a> |
                        <iframe style="margin-left: 2px; margin-right: -20px; margin-bottom:-5px; opacity: 0.55;"
                            frameborder="0" scrolling="0" width="100px" height="20px"
                            src="https://ghbtns.com/github-btn.html?user=luohj29&repo=luohj29.github.io&type=star&count=true">
                        </iframe></text><br>
                    <text style="opacity: 0.6; font-size: 13px;">Improved from the theme by <a
                            href="http://huangxuan.me/" target="_blank">Hux Blog</a></text>
                    <!-- </text> -->
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src=" /js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src=" /js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src=" /js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src=" /js/snackbar.js "></script>
<script src=" /js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->





<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog(selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->



    <!-- Image to hack wechat -->
    <img src="/img/icon_wechat.png" width="0" height="0" />
    <!-- Migrate from head to bottom, no longer block render and still work -->
</body>

</html>